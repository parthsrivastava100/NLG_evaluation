# -*- coding: utf-8 -*-
"""BERTScore.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nyxi4EB4YsofXwjwufUMzVj0Hv2dpqmH
"""

# THIS IS A NON-GPU CODE
# IMPORTING THE LIBRARIES
score = []
import os
os.system('pip install bert-embedding')
os.system('pip install -r requirements_bertscore.txt')
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from bert_embedding import BertEmbedding
bert_embedding = BertEmbedding(model='bert_24_1024_16', dataset_name='book_corpus_wiki_en_cased')

def get_embedding_matrix(ref_sent,cand_sent):
  ref_array=np.ones((len(ref_sent),1024))
  cand_array=np.ones((len(cand_sent),1024))
  for index,ref in enumerate(ref_sent):
    b=np.array(ref[1])
    if b.shape[0] != 1:
      b=(np.sum(np.array(ref[1]),axis=0))/(np.array(ref[1])).shape[0]
    ref_array[index,:]=np.array(b)
  for index,cand in enumerate(cand_sent):
    b=np.array(cand[1])
    if b.shape[0] != 1:
      b=(np.sum(np.array(cand[1]),axis=0))/(np.array(cand[1])).shape[0]
    cand_array[index,:]=np.array(b)
  return ref_array,cand_array

def get_weights(l):
    weights=np.ones((1,l))
    return weights


def cos_similarity(ref_array,cand_array):
  result=cosine_similarity(cand_array,Y=ref_array)
  return result

def final_score(result,weights):
  final_score=np.sum(np.multiply(result,weights))/(np.sum(weights))
  return final_score

def bertt(a,b):
  # INPUTTING THE SENTENCES AND GETTING THE SENTENCE EMBEDDINGS
  ref_sent_list = list(a.split('.'))
  cand_sent_list = list(b.split('.'))
  assert len(ref_sent_list)==len(cand_sent_list) , "Number of refrence and candidate sentences should be equal"
  for i in range(0,len(ref_sent_list)-1):
    ref_sent=((ref_sent_list[i]).strip()).split(' ')
    cand_sent=((cand_sent_list[i]).strip()).split(' ')
    # GETTING THE EMBEDDINGS IN A MATRIX
    ref_array,cand_array = get_embedding_matrix(bert_embedding(ref_sent),bert_embedding(cand_sent)) 
    # GETTING THE PAIR-WISE COSINE SIMILARITY
    result = cos_similarity(ref_array,cand_array)
    # APPLY MAX-POOLING ON THE RESULT
    result=(np.max(result,axis=1))
    # TAKING WEIGHTS INPUT
    weights=get_weights(len(cand_sent))  
    #CALCULATING FINAL SCORE
    f_score=final_score(result,weights)
    score.append(f_score)
import pandas as pd
train=pd.read_csv('/home/parth/NLG_evaluation/datasets/SICK_train - SICK_train.csv')
ref_list=[]
for i in range(4500):
  if (train['sentence_A'][i]).endswith('.'):
    ref_list.append((train['sentence_A'][i]))
  else :
    ref_list.append((train['sentence_A'][i]+'.'))
hyp_list=[]
for i in range(4500):
  if (train['sentence_B'][i]).endswith('.'):
    hyp_list.append((train['sentence_B'][i]))
  else :
    hyp_list.append((train['sentence_B'][i]+'.'))  
for i in range(4500):
  bertt(ref_list[i],hyp_list[i])
print(score)

