# -*- coding: utf-8 -*-
"""embedding_average.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Pdb8rDMR8IZeTf0RAn9JLyq5rWJD_O4
"""
import os
os.system('wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip')
os.system('unzip glove.6B.zip')
import numpy as np
import math
import itertools
model = model.WordEmbedding(model_fn="./glove.6B/glove.6B.300d.txt")
embeddings_dict = {}
with open("glove.6B.300d.txt", 'r') as f:
  for line in f:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:], "float32")
    embeddings_dict[word] = vector

def process_input():
  import numpy as np
  max_score=0
  score_sum=0
  for i,hyp in enumerate(hypo_list[:-1]):
    hyp_embd= np.zeros((len(hyp.split(' ')),300))
    for index,j in enumerate(hyp.split(' ')):
      print(embeddings_dict[str(j)])
      hyp_embed[index,:]=embeddings_dict[j]
    hyp_embed=np.sum(hyp_embed,axis=0)/len(hyp.split(' '))
    for p,ref in enumerate(ref_list[:-1]):
      ref_embd=np.zeros((len(ref.split(' ')),300))
      for alpha,q in enumerate(ref.split(' ')):
        ref_embd[alpha,:]=embeddings_dict[q]
      ref_embd=np.sum(ref_embd,axis=0)/len(ref.split(' '))
      c=0
      for l in range(0,300):
        c+=hyp_embd[0,l]*ref_embd[0,l]
      cosine = c/np.sqrt(np.sum(np.multiply(hyp_embd,hyp_embd),axis=0)*np.sum(np.multiply(ref_embd,ref_embd),axis=0))
      max_score=max(max_score,cosine)
    print(max_score)
    score_sum += max_score
    max_score=0
  return score_sum

if __name__=='__main__':
  ref_list=input().split('.')
  hypo_list=input().split('.')
  score=process_input(ref_list,hypo_list)
  print(score/(len(hypo_list)-1))